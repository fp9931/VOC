import os
import numpy as np
import pandas as pd
import itertools
from collections import Counter

from mrmr import mrmr_classif
from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit, RandomizedSearchCV, train_test_split
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, confusion_matrix, recall_score, precision_score, accuracy_score, root_mean_squared_error, root_mean_squared_log_error, r2_score, roc_auc_score
from sklearn.svm import SVC, SVR
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from xgboost import XGBClassifier, XGBRegressor
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.feature_selection import RFECV, RFE
from sklearn.linear_model import LogisticRegression, LinearRegression



general_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
features_path = os.path.join(general_path, 'Features')
results_path = os.path.join(general_path, 'Results')

# Load the cleaned dataframes
df_complete = pd.read_excel(os.path.join(features_path, 'complete_clean.xlsx'))

columns_to_drop_complete = ['subjid', 'category', 'sex', 'ALSFRS-R_SpeechSubscore', 'ALSFRS-R_SwallowingSubscore', 'PUMNS_BulbarSubscore']
task_complete = ['_a','_e', '_i', '_o', '_u', '_k', '_p', '_t']

# Filter ALS patients and drop unnecessary columns
als_df_complete = df_complete[df_complete['category'] == 'ALS']

y_speech = als_df_complete['ALSFRS-R_SpeechSubscore'].values

id = als_df_complete['subjid'].values
print(id)
# Compute chance level
chance_level_speech = max(Counter(y_speech).values()) / len(y_speech)
print(f"Chance level: {chance_level_speech:.2f}")

df_clean = als_df_complete.drop(columns=columns_to_drop_complete)

X_df = df_clean.copy()
X = X_df.values

# Normal (4) vs Impaired (0, 1, 2, 3)
count_normal = (y_speech == 4).sum()
count_impaired = (y_speech < 4).sum()
proportion_impaired = count_impaired / (count_normal + count_impaired) if (count_normal + count_impaired) > 0 else 0

y = np.where(y_speech == 4, 0, 1) # 0 normal, 1 impaired

models = [
    {"name": "SVM", "model": SVC(class_weight='balanced'), "parameters": {
        'C': [0.0001, 0.01, 0.02, 0.1, 0.2, 1, 2, 10, 20, 100, 1000],
        'kernel': ['linear', 'rbf', 'sigmoid', 'poly'],
        'gamma': [0.0001, 0.001, 0.01, 0.1, 1],
        'degree': [2, 3, 4],
    }},
    {"name": "RF", "model": RandomForestClassifier(random_state=42, n_jobs=-1, class_weight='balanced'), "parameters": {
        'n_estimators': [10, 20, 30, 40, 50, 60, 70, 100, 150, 200, 300],
        'max_depth': [None, 2, 5, 7, 10, 20, 30],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
    }},
    {"name": "XGB", "model": XGBClassifier(random_state=42, n_jobs=-1), "parameters": {
        'n_estimators': [10, 20, 30, 40, 50, 100, 200],
        'max_depth': [2, 3, 5, 7, 9],
        'learning_rate': [0.01, 0.1, 0.2, 0.5, 0.7, 1.0],
        'subsample': [0.1, 0.2, 0.3, 0.5, 0.7, 1.0],
    }},
    {"name": "KNN", "model": KNeighborsClassifier(n_jobs=-1), "parameters": {
        'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10, 15],
        'weights': ['uniform', 'distance'],
        'metric': ['euclidean', 'manhattan', 'minkowski'],
    }},
    {"name": "MLP", "model": MLPClassifier(random_state=42, max_iter=1000, early_stopping=True, n_iter_no_change=10), "parameters": {
        'hidden_layer_sizes': [(64,), (32,), (16,), (8,), (64,32), (32, 16), (16, 8)],
        'activation': ['relu', 'tanh', 'logistic'],
        'alpha': [0.0001, 0.001],
    }},
]

inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
out_cv = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, test_idx in out_cv.split(X, y):
    X_train, y_train = X[train_idx], y[train_idx]
    X_test, y_test = X[test_idx], y[test_idx]
    # Verify proportions in train and test sets
    count_normal_train = (y_train == 0).sum()
    count_impaired_train = (y_train == 1).sum()
    count_normal_test = (y_test == 0).sum()
    count_impaired_test = (y_test == 1).sum()
    print(f"Train set: {count_normal_train} normal, {count_impaired_train} impaired")
    print(f"Test set: {count_normal_test} normal, {count_impaired_test} impaired")

    #Impute missing values
    imputer = IterativeImputer(max_iter=10, random_state=42)
    X_train = imputer.fit_transform(X_train)
    X_test = imputer.transform(X_test)

    # Scale the data
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    model_predictions = {}
    model_names = []

    # Train and evaluate each model
    for model_info in models:
        model_name = model_info['subjid']
        model_class = model_info['model'].__class__
        model_parameters = model_info['parameters']

        best_f1_validation = -np.inf

        for params in itertools.product(*model_parameters.values()):
            params = dict(zip(model_parameters.keys(), params))

            f1_validation = []
            for inner_folder_idx, (train_idx, val_idx) in enumerate(inner_cv.split(X_train, y_train)):
                X_inner_train, y_inner_train = X_train[train_idx], y_train[train_idx]
                X_inner_val, y_inner_val = X_train[val_idx], y_train[val_idx]

                estimator = LogisticRegression(max_iter=1000)
                X_task_df = pd.DataFrame(X_inner_train, columns=X_df.columns)
                selected_features = []
                for id_task in task_complete:
                    features = [col for col in X_df.columns if col.endswith(id_task)]
                    X_task = X_task_df[features]
                    y_task = pd.Series(y_inner_train, name='ALSFRS-R_SpeechSubscore')

                    selector = RFE(estimator=estimator, step=1, n_features_to_select=5)
                    selected_mask = selector.fit(X_task.values, y_task.values).support_
                    selected_features.extend([features[i] for i in range(len(features)) if selected_mask[i]])

                    selected_indices = [X_df.columns.get_loc(col) for col in selected_features]
                    X_inner_train = X_inner_train[:, selected_indices]
                    X_inner_val = X_inner_val[:, selected_indices]

                    model = model_class(**params)
                    model.fit(X_inner_train, y_inner_train)
                    y_inner_pred = model.predict(X_inner_val)

                    validation_f1 = f1_score(y_inner_val, y_inner_pred)
                    f1_validation.append(validation_f1)

            mean_f1_validation = np.median(f1_validation)
            if mean_f1_validation > best_f1_validation:
                best_f1_validation = mean_f1_validation
                best_params = params

        best_model = model_class(**best_params)

        estimator = LogisticRegression(max_iter=1000)
        X_task_df = pd.DataFrame(X_train, columns=X_df.columns)
        selected_features = []
        for id_task in task_complete:
            features = [col for col in X_df.columns if col.endswith(id_task)]
            X_task = X_task_df[features]
            y_task = pd.Series(y_train, name='ALSFRS-R_SpeechSubscore')

            selector = RFE(estimator=estimator, step=1, n_features_to_select=5)
            selected_mask = selector.fit(X_task.values, y_task.values).support_
            selected_features.extend([features[i] for i in range(len(features)) if selected_mask[i]])

        selected_indices = [X_df.columns.get_loc(col) for col in selected_features]
        X_train_selected = X_train[:, selected_indices]
        X_test_selected = X_test[:, selected_indices]
        best_model.fit(X_train_selected, y_train)

        if hasattr(best_model, "predict_proba"):
            y_prob = best_model.predict_proba(X_test_selected)[:, 1]  # ProbabilitÃ  della classe "1" (impaired)
        else:
            # Per modelli che non supportano predict_proba (es. SVM con kernel='linear'), fallback con decision_function
            y_prob = best_model.decision_function(X_test_selected)
            y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min())  # Normalize to [0, 1]

        model_predictions[model_name] = y_prob
        model_names.append(model_name)

        print(f"Model: {model_name}")

    print("\nLate Fusion - Grid Search on weights")

    weights = np.arange(0, 1.1, 0.1)
    combinations = [w for w in itertools.product(weights, repeat=len(model_names)) if np.isclose(sum(w), 1.0)]

    best_f1 = 0
    best_combo = None
    best_preds = None

    for combo in combinations:
        # Calcolo della predizione pesata
        fused = np.zeros_like(list(model_predictions.values())[0])
        for weight, name in zip(combo, model_names):
            fused += weight * model_predictions[name]

        y_fused_pred = (fused >= 0.5).astype(int)
        score = f1_score(y_test, y_fused_pred)

        if score > best_f1:
            best_f1 = score
            best_combo = combo
            best_preds = y_fused_pred

    # Mostra risultati
    print(f"Best fusion weights: {dict(zip(model_names, best_combo))}")
    print(f"Best F1-score from late fusion: {best_f1:.4f}")
